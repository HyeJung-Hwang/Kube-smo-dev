{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace Dataset Visualization (N=100)\n",
    "\n",
    "Visualize the Azure trace dataset and ShareGPT prompts used by trace-sharegpt-hybrid.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Azure Trace (Timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Azure trace\n",
    "azure_trace_file = '/home/skt6g/AI-RAN/KubeSMO/data/AzureLLMInferenceTrace_conv_1week.csv'\n",
    "N = 100\n",
    "\n",
    "# Read first N rows\n",
    "df_azure = pd.read_csv(azure_trace_file, nrows=N)\n",
    "\n",
    "print(f\"ðŸ“Š Loaded {len(df_azure)} Azure trace requests\")\n",
    "print(f\"\\nColumns: {df_azure.columns.tolist()}\")\n",
    "df_azure.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parse Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_timestamp(ts_str):\n",
    "    \"\"\"Parse Azure timestamp\"\"\"\n",
    "    ts_str = ts_str.strip()\n",
    "    \n",
    "    # Remove Z suffix\n",
    "    if ts_str.endswith('Z'):\n",
    "        ts_str = ts_str[:-1]\n",
    "    \n",
    "    # Remove timezone offset\n",
    "    if '+' in ts_str:\n",
    "        ts_str = ts_str.split('+')[0]\n",
    "    elif ts_str.count('-') > 2:\n",
    "        parts = ts_str.rsplit('-', 1)\n",
    "        if ':' in parts[1]:\n",
    "            ts_str = parts[0]\n",
    "    \n",
    "    # Handle fractional seconds\n",
    "    if '.' in ts_str:\n",
    "        parts = ts_str.split('.')\n",
    "        fractional = parts[1][:6].ljust(6, '0')\n",
    "        ts_str = f\"{parts[0]}.{fractional}+00:00\"\n",
    "    else:\n",
    "        ts_str = f\"{ts_str}+00:00\"\n",
    "    \n",
    "    return datetime.fromisoformat(ts_str)\n",
    "\n",
    "# Parse timestamps\n",
    "df_azure['timestamp'] = df_azure['TIMESTAMP'].apply(parse_timestamp)\n",
    "\n",
    "# Calculate inter-arrival times\n",
    "df_azure['inter_arrival_sec'] = df_azure['timestamp'].diff().dt.total_seconds()\n",
    "\n",
    "# Calculate time from start\n",
    "start_time = df_azure['timestamp'].iloc[0]\n",
    "df_azure['time_from_start_sec'] = (df_azure['timestamp'] - start_time).dt.total_seconds()\n",
    "\n",
    "print(f\"\\nâ±ï¸  Trace Duration: {df_azure['time_from_start_sec'].max():.1f} seconds\")\n",
    "print(f\"   ({df_azure['time_from_start_sec'].max()/60:.2f} minutes)\")\n",
    "print(f\"\\nðŸ“ˆ Inter-arrival Time Statistics:\")\n",
    "print(f\"   Mean: {df_azure['inter_arrival_sec'].mean():.2f} sec\")\n",
    "print(f\"   Min:  {df_azure['inter_arrival_sec'].min():.2f} sec\")\n",
    "print(f\"   Max:  {df_azure['inter_arrival_sec'].max():.2f} sec\")\n",
    "print(f\"   p50:  {df_azure['inter_arrival_sec'].median():.2f} sec\")\n",
    "print(f\"   p95:  {df_azure['inter_arrival_sec'].quantile(0.95):.2f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load ShareGPT Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ShareGPT\n",
    "sharegpt_file = '/home/skt6g/AI-RAN/KubeSMO/data/sg_90k_part1.json'\n",
    "\n",
    "with open(sharegpt_file, 'r', encoding='utf-8') as f:\n",
    "    sharegpt_data = json.load(f)\n",
    "\n",
    "# Extract prompts\n",
    "prompts = []\n",
    "for item in sharegpt_data:\n",
    "    conv = item.get('conversations', [])\n",
    "    for msg in conv:\n",
    "        if msg.get('from') == 'human':\n",
    "            prompt = msg.get('value', '').strip()\n",
    "            if prompt and len(prompt) > 10:\n",
    "                prompts.append(prompt)\n",
    "\n",
    "print(f\"\\nðŸ“ Loaded {len(prompts)} ShareGPT prompts\")\n",
    "\n",
    "# Sample the first N prompts (cycling behavior)\n",
    "selected_prompts = [prompts[i % len(prompts)] for i in range(N)]\n",
    "prompt_lengths = [len(p) for p in selected_prompts]\n",
    "\n",
    "print(f\"\\nðŸ“ Prompt Length Statistics (first {N} used):\")\n",
    "print(f\"   Mean: {np.mean(prompt_lengths):.1f} chars\")\n",
    "print(f\"   Min:  {np.min(prompt_lengths)} chars\")\n",
    "print(f\"   Max:  {np.max(prompt_lengths)} chars\")\n",
    "print(f\"   p50:  {np.median(prompt_lengths):.1f} chars\")\n",
    "print(f\"   p95:  {np.percentile(prompt_lengths, 95):.1f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Request Arrival Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. Request arrivals over time\n",
    "axes[0, 0].scatter(df_azure['time_from_start_sec'], \n",
    "                   range(len(df_azure)), \n",
    "                   alpha=0.6, s=30)\n",
    "axes[0, 0].set_xlabel('Time from Start (seconds)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Request Number', fontsize=12)\n",
    "axes[0, 0].set_title(f'Request Arrival Pattern (N={N})', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Inter-arrival time distribution\n",
    "axes[0, 1].hist(df_azure['inter_arrival_sec'].dropna(), bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].axvline(df_azure['inter_arrival_sec'].mean(), \n",
    "                   color='red', linestyle='--', linewidth=2, label=f'Mean: {df_azure[\"inter_arrival_sec\"].mean():.2f}s')\n",
    "axes[0, 1].set_xlabel('Inter-arrival Time (seconds)', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 1].set_title('Inter-arrival Time Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Cumulative requests\n",
    "axes[1, 0].plot(df_azure['time_from_start_sec'], range(1, len(df_azure)+1), linewidth=2)\n",
    "axes[1, 0].set_xlabel('Time from Start (seconds)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Cumulative Requests', fontsize=12)\n",
    "axes[1, 0].set_title('Cumulative Request Arrivals', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Prompt length distribution\n",
    "axes[1, 1].hist(prompt_lengths, bins=30, alpha=0.7, edgecolor='black', color='green')\n",
    "axes[1, 1].axvline(np.mean(prompt_lengths), \n",
    "                   color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(prompt_lengths):.1f} chars')\n",
    "axes[1, 1].set_xlabel('Prompt Length (characters)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 1].set_title('ShareGPT Prompt Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('trace_dataset_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¾ Saved visualization to: trace_dataset_visualization.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Request Rate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate request rate in time windows\n",
    "window_size = 10  # seconds\n",
    "max_time = df_azure['time_from_start_sec'].max()\n",
    "time_bins = np.arange(0, max_time + window_size, window_size)\n",
    "\n",
    "request_counts = []\n",
    "for i in range(len(time_bins) - 1):\n",
    "    count = ((df_azure['time_from_start_sec'] >= time_bins[i]) & \n",
    "             (df_azure['time_from_start_sec'] < time_bins[i+1])).sum()\n",
    "    request_counts.append(count)\n",
    "\n",
    "request_rates = [count / window_size for count in request_counts]\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.bar(time_bins[:-1], request_rates, width=window_size*0.9, alpha=0.7, edgecolor='black')\n",
    "plt.axhline(np.mean(request_rates), color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {np.mean(request_rates):.2f} req/s')\n",
    "plt.xlabel('Time from Start (seconds)', fontsize=12)\n",
    "plt.ylabel('Request Rate (requests/second)', fontsize=12)\n",
    "plt.title(f'Request Rate Over Time (Window={window_size}s)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('request_rate_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Request Rate Statistics ({window_size}s windows):\")\n",
    "print(f\"   Mean:  {np.mean(request_rates):.2f} req/s\")\n",
    "print(f\"   Min:   {np.min(request_rates):.2f} req/s\")\n",
    "print(f\"   Max:   {np.max(request_rates):.2f} req/s\")\n",
    "print(f\"   p50:   {np.median(request_rates):.2f} req/s\")\n",
    "print(f\"   p95:   {np.percentile(request_rates, 95):.2f} req/s\")\n",
    "print(f\"\\nðŸ’¾ Saved visualization to: request_rate_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample Prompts Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“ Sample Prompts (first 5):\\n\")\n",
    "print(\"=\"*80)\n",
    "for i, prompt in enumerate(selected_prompts[:5]):\n",
    "    print(f\"\\nPrompt {i+1} ({len(prompt)} chars):\")\n",
    "    print(\"-\"*80)\n",
    "    # Show first 200 chars\n",
    "    preview = prompt[:200] + \"...\" if len(prompt) > 200 else prompt\n",
    "    print(preview)\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Total Requests',\n",
    "        'Trace Duration (sec)',\n",
    "        'Trace Duration (min)',\n",
    "        'Mean Inter-arrival (sec)',\n",
    "        'Mean Request Rate (req/s)',\n",
    "        'Mean Prompt Length (chars)',\n",
    "        'Max Prompt Length (chars)',\n",
    "        'Total ShareGPT Prompts Available'\n",
    "    ],\n",
    "    'Value': [\n",
    "        N,\n",
    "        f\"{df_azure['time_from_start_sec'].max():.1f}\",\n",
    "        f\"{df_azure['time_from_start_sec'].max()/60:.2f}\",\n",
    "        f\"{df_azure['inter_arrival_sec'].mean():.2f}\",\n",
    "        f\"{np.mean(request_rates):.2f}\",\n",
    "        f\"{np.mean(prompt_lengths):.1f}\",\n",
    "        f\"{np.max(prompt_lengths)}\",\n",
    "        len(prompts)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nðŸ“Š Dataset Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Data for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create analysis dataframe\n",
    "analysis_df = pd.DataFrame({\n",
    "    'request_id': range(N),\n",
    "    'timestamp': df_azure['timestamp'],\n",
    "    'time_from_start_sec': df_azure['time_from_start_sec'],\n",
    "    'inter_arrival_sec': df_azure['inter_arrival_sec'],\n",
    "    'prompt_length_chars': prompt_lengths,\n",
    "    'prompt_preview': [p[:100] for p in selected_prompts]  # First 100 chars\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'trace_dataset_analysis.csv'\n",
    "analysis_df.to_csv(output_file, index=False)\n",
    "print(f\"\\nðŸ’¾ Saved analysis data to: {output_file}\")\n",
    "\n",
    "analysis_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
