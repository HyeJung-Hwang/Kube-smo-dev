# vLLM Auto-Scaling Benchmark

vLLM ë©”íŠ¸ë¦­ì„ ê¸°ë°˜ìœ¼ë¡œ ìë™ìœ¼ë¡œ Podë¥¼ ìŠ¤ì¼€ì¼ ì•„ì›ƒí•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ ë„êµ¬ì…ë‹ˆë‹¤.

## ê°œìš”

ì´ ë„êµ¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë™ì‘í•©ë‹ˆë‹¤:

1. **ì²« ë²ˆì§¸ Podì—ì„œ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘**
2. **vLLM ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§**: `/metrics` ì—”ë“œí¬ì¸íŠ¸ì—ì„œ `vllm:num_requests_waiting` ë©”íŠ¸ë¦­ ì§ì ‘ í™•ì¸
3. **ìŠ¤ì¼€ì¼ ì•„ì›ƒ ì¡°ê±´**: ë©”íŠ¸ë¦­ì´ 1 ì´ìƒì¸ ê°’ì´ 2ë²ˆ ì´ìƒ ë‚˜ì˜¤ë©´
4. **ë‘ ë²ˆì§¸ Pod ìë™ ë°°í¬**: Helmìœ¼ë¡œ ìƒˆ Pod ë°°í¬
5. **íŠ¸ë˜í”½ ë¶„ì‚°**: ì´í›„ ìš”ì²­ì´ ë‘ Podë¡œ ë¶„ì‚°ë¨

## íŠ¹ì§•

- **Prometheus ì„œë²„ ë¶ˆí•„ìš”**: vLLM podì˜ `/metrics` ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ì§ì ‘ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- **ê°„ë‹¨í•œ ì„¤ì •**: Pod í¬íŠ¸í¬ì›Œë”©ë§Œìœ¼ë¡œ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥
- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: 5ì´ˆë§ˆë‹¤ ë©”íŠ¸ë¦­ ì²´í¬ ë° ìë™ ìŠ¤ì¼€ì¼ ê²°ì •

## ì‚¬ì „ ì¤€ë¹„

### 1. ì²« ë²ˆì§¸ Pod ë°°í¬

```bash
helm install test-1 /home/skt6g/AI-RAN/KubeSMO/workload/vllm-server \
  --set nodeName=sys-221he-tnr \
  --set gpuResource=nvidia.com/mig-1g.12gb \
  --set-string secret.hfApiToken=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 \
  --set server.modelPath=/root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct \
  --set server.port=8001 \
  --set server.service.port=8001 \
  --set server.service.targetPort=8001
```

### 2. ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸ í™•ì¸

vLLMì˜ ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸ê°€ ì •ìƒ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸:

```bash
# ë©”íŠ¸ë¦­ í™•ì¸
curl http://0.0.0.0:8001/metrics | grep num_requests_waiting

# ë˜ëŠ” ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
./monitor_metrics.sh 0.0.0.0 8001
```

## ì‚¬ìš©ë²•

### ê¸°ë³¸ ì‹¤í–‰

```bash
python3 autoscale_benchmark.py \
  --model /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct \
  --host 0.0.0.0 \
  --base-port 8001 \
  --num-prompts 1000 \
  --random-input-len 512 \
  --random-output-len 256 \
  --request-rate 5
```

### ì£¼ìš” íŒŒë¼ë¯¸í„°

#### ë²¤ì¹˜ë§ˆí¬ ì„¤ì •
- `--model`: ëª¨ë¸ ê²½ë¡œ (í•„ìˆ˜)
- `--host`: vLLM ì„œë²„ í˜¸ìŠ¤íŠ¸ (ê¸°ë³¸: 0.0.0.0)
- `--base-port`: ì²« ë²ˆì§¸ ì„œë²„ í¬íŠ¸ (ê¸°ë³¸: 8001)
- `--num-prompts`: ì´ í”„ë¡¬í”„íŠ¸ ìˆ˜ (ê¸°ë³¸: 1000)
- `--random-input-len`: ì…ë ¥ í† í° ê¸¸ì´ (ê¸°ë³¸: 512)
- `--random-output-len`: ì¶œë ¥ í† í° ê¸¸ì´ (ê¸°ë³¸: 256)
- `--request-rate`: ì´ˆë‹¹ ìš”ì²­ ìˆ˜ (ê¸°ë³¸: 5)

#### ëª¨ë‹ˆí„°ë§ ì„¤ì •
- `--check-interval`: ë©”íŠ¸ë¦­ ì²´í¬ ì£¼ê¸° (ì´ˆ, ê¸°ë³¸: 5)

#### Helm ë°°í¬ ì„¤ì •
- `--chart-path`: Helm ì°¨íŠ¸ ê²½ë¡œ
- `--node-name`: Kubernetes ë…¸ë“œ ì´ë¦„
- `--gpu-resource`: GPU ë¦¬ì†ŒìŠ¤ íƒ€ì…
- `--hf-token`: HuggingFace API í† í°

#### ì¶”ê°€ ì˜µì…˜
- `--run-distributed`: ìŠ¤ì¼€ì¼ ì•„ì›ƒ í›„ ë¶„ì‚° ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€ ì‹¤í–‰

### ê³ ê¸‰ ì‚¬ìš© ì˜ˆì œ

```bash
# ë†’ì€ request rateë¡œ ìŠ¤ì¼€ì¼ ì•„ì›ƒ í…ŒìŠ¤íŠ¸
python3 autoscale_benchmark.py \
  --model /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct \
  --host 0.0.0.0 \
  --base-port 8001 \
  --num-prompts 2000 \
  --random-input-len 1024 \
  --random-output-len 512 \
  --request-rate 10 \
  --check-interval 3 \
  --run-distributed
```

## ì‹¤í–‰ íë¦„

### 1ë‹¨ê³„: ë‹¨ì¼ Pod ë²¤ì¹˜ë§ˆí¬
```
[10:30:00] num_requests_waiting: 0.0
[10:30:05] num_requests_waiting: 1.0
  â†’ Threshold exceeded (1/2)
[10:30:10] num_requests_waiting: 2.0
  â†’ Threshold exceeded (2/2)
  âœ“ Scale condition met!

ğŸš€ SCALING OUT at request 250/1000
```

### 2ë‹¨ê³„: ë‘ ë²ˆì§¸ Pod ë°°í¬
```
============================================================
Deploying new pod: test-2 on port 8002
============================================================
Command: helm install test-2 /home/skt6g/AI-RAN/KubeSMO/workload/vllm-server ...

Waiting for server at 0.0.0.0:8002 to be ready...
âœ“ Server at 0.0.0.0:8002 is ready!
```

### 3ë‹¨ê³„: ê²°ê³¼ ì¶œë ¥
```
============================================================
Single Pod Benchmark Results
============================================================
Duration:             120.45s
Completed requests:   1000
Failed requests:      0
Request throughput:   8.30 req/s
Token throughput:     2123.45 tok/s
============================================================

ğŸ¯ Scaled out! Now running on 2 pods
```

## ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­

ìŠ¤í¬ë¦½íŠ¸ê°€ ëª¨ë‹ˆí„°ë§í•˜ëŠ” vLLM ë©”íŠ¸ë¦­:

```bash
# /metrics ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ë©”íŠ¸ë¦­
vllm:num_requests_waiting{engine="0",model_name="..."}

# ë˜ëŠ”
vllm_num_requests_waiting{engine="0",model_name="..."}
```

### ë©”íŠ¸ë¦­ í™•ì¸ ë°©ë²•
```bash
# ì§ì ‘ í™•ì¸
curl http://0.0.0.0:8001/metrics | grep num_requests_waiting

# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
./monitor_metrics.sh 0.0.0.0 8001
```

### ìŠ¤ì¼€ì¼ ì•„ì›ƒ ì¡°ê±´
- ë©”íŠ¸ë¦­ ê°’ â‰¥ 1 ì´ **2ë²ˆ ì—°ì†** ë°œìƒ ì‹œ

## ì¶œë ¥ íŒŒì¼

ì—†ìŒ (ì½˜ì†” ì¶œë ¥ë§Œ)

## ë¬¸ì œ í•´ê²°

### ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸ ì—°ê²° ì‹¤íŒ¨
```bash
# vLLM ì„œë²„ health ì²´í¬
curl http://0.0.0.0:8001/health

# ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸ í™•ì¸
curl http://0.0.0.0:8001/metrics

# íŠ¹ì • ë©”íŠ¸ë¦­ í™•ì¸
curl http://0.0.0.0:8001/metrics | grep vllm
```

### Helm ë°°í¬ ì‹¤íŒ¨
```bash
# Helm ì„¤ì¹˜ í™•ì¸
helm list

# ì´ì „ ë¦´ë¦¬ìŠ¤ ì‚­ì œ
helm uninstall test-2

# Kubernetes ê¶Œí•œ í™•ì¸
kubectl auth can-i create pods
```

### Podê°€ Ready ìƒíƒœê°€ ì•ˆ ë¨
```bash
# Pod ìƒíƒœ í™•ì¸
kubectl get pods -l app=vllm-server

# Pod ë¡œê·¸ í™•ì¸
kubectl logs test-2-vllm-server-xxx

# Health ì—”ë“œí¬ì¸íŠ¸ ì§ì ‘ í™•ì¸
curl http://0.0.0.0:8002/health
```

## ì½”ë“œ êµ¬ì¡°

```
autoscale_benchmark.py
â”œâ”€â”€ MetricsMonitor         # vLLM /metrics ì—”ë“œí¬ì¸íŠ¸ ëª¨ë‹ˆí„°ë§
â”‚   â”œâ”€â”€ parse_prometheus_metrics()  # Prometheus í˜•ì‹ íŒŒì‹±
â”‚   â”œâ”€â”€ query_num_requests_waiting()  # ë©”íŠ¸ë¦­ ì¿¼ë¦¬
â”‚   â””â”€â”€ check_should_scale()        # ìŠ¤ì¼€ì¼ ì¡°ê±´ ì²´í¬
â”œâ”€â”€ HelmDeployer           # Helmì„ í†µí•œ Pod ë°°í¬
â”œâ”€â”€ run_benchmark_with_monitoring  # ëª¨ë‹ˆí„°ë§í•˜ë©° ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
â”œâ”€â”€ run_distributed_benchmark      # ë¶„ì‚° ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
â””â”€â”€ main                   # ë©”ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
```

## ì£¼ìš” ê¸°ëŠ¥

### 1. ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§
- vLLM `/metrics` ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ì§ì ‘ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- Prometheus í˜•ì‹ íŒŒì‹±
- `num_requests_waiting` ê°’ ì¶”ì 
- ì„ê³„ê°’ ì´ˆê³¼ íšŸìˆ˜ ì¹´ìš´íŒ…

### 2. ë™ì  ìŠ¤ì¼€ì¼ ì•„ì›ƒ
- ì¡°ê±´ ì¶©ì¡± ì‹œ ìë™ìœ¼ë¡œ Helm ë°°í¬
- ìƒˆ Podì˜ Ready ìƒíƒœ ëŒ€ê¸°
- ë°°í¬ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ í•¸ë“¤ë§

### 3. íŠ¸ë˜í”½ ë¶„ì‚°
- ìŠ¤ì¼€ì¼ ì•„ì›ƒ í›„ ë‚¨ì€ ìš”ì²­ì€ ë‹¨ì¼ Podë¡œ ê³„ì† ì „ì†¡
- `--run-distributed` ì˜µì…˜ ì‚¬ìš© ì‹œ ì¶”ê°€ë¡œ ë¶„ì‚° ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰

## ì œí•œì‚¬í•­

1. **í˜„ì¬ëŠ” ìµœëŒ€ 2ê°œ Podê¹Œì§€ ìŠ¤ì¼€ì¼ ì•„ì›ƒ**
   - í•„ìš”ì‹œ ì½”ë“œ ìˆ˜ì •ìœ¼ë¡œ Nê°œê¹Œì§€ í™•ì¥ ê°€ëŠ¥

2. **ìŠ¤ì¼€ì¼ ì¸(Scale In)ì€ ë¯¸ì§€ì›**
   - PodëŠ” ìë™ìœ¼ë¡œ ì œê±°ë˜ì§€ ì•ŠìŒ

3. **ë‹¨ìˆœ Round-Robin ë¶„ì‚°**
   - ê³ ê¸‰ ë¡œë“œ ë°¸ëŸ°ì‹± ì•Œê³ ë¦¬ì¦˜ ì—†ìŒ

## í–¥í›„ ê°œì„  ì‚¬í•­

- [ ] Nê°œ Podë¡œ í™•ì¥ ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì •
- [ ] ìŠ¤ì¼€ì¼ ì¸ ê¸°ëŠ¥ ì¶”ê°€
- [ ] ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§ (latency, throughput ë“±)
- [ ] ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
- [ ] ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ í†µí•©
- [ ] Weighted round-robin ë¡œë“œ ë°¸ëŸ°ì‹±
